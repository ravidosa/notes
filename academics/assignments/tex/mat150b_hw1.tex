\documentclass{article}
\usepackage{assignment_preamble}

\title{Homework 1}
\author{Ravi Kini}
\date{January 24, 2024}

\begin{document}

\maketitle

\problem
\subsection*{Part (a)}
Let $M$ be an $n \times n$ matrix, with eigenvectors $u = \left\{u_1, \ldots, u_n\right\}$ and corresponding eigenvalues $\lambda = \left\{\lambda_1, \ldots, \lambda_n\right\}$. Then, since $Mu_i = \lambda_iu_i$:
\begin{equation}
    \begin{split}
        M\begin{bmatrix}\vert & \vert &  & \vert \\u_1 &  u_2 & \hdots &  u_n\\\vspace{.15 cm}\vert & \vert &  & \vert\end{bmatrix} & = \begin{bmatrix}\vert & \vert &  & \vert \\\lambda_1u_1 &  \lambda_2u_2 & \hdots &  \lambda_nu_n\\\vspace{.15 cm}\vert & \vert &  & \vert\end{bmatrix}
    \end{split}
\end{equation}
\subsection*{Part (b)}
For some small $n$:
\begin{equation}
    \begin{split}
        M & = \begin{bmatrix}
            1 & 2 \\ 2 & 1
        \end{bmatrix} \\
        \lambda & = \left\{ -1, 3 \right\} \\
        u & = \left\{ \begin{bmatrix}
            1 \\ 1
        \end{bmatrix},  \begin{bmatrix}
            -1 \\ 1
        \end{bmatrix} \right\} \\
        \begin{bmatrix}
            1 & 2 \\ 2 & 1
        \end{bmatrix}\begin{bmatrix}
            1 & -1 \\ 1 & 1
        \end{bmatrix} & = \begin{bmatrix}
            3 & 1 \\ 3 & -1
        \end{bmatrix} = \begin{bmatrix}
            3\begin{bmatrix}
            1 \\ 1
        \end{bmatrix} & -1\begin{bmatrix}
            -1 \\ 1
        \end{bmatrix}
        \end{bmatrix}
    \end{split}
\end{equation}
\begin{equation}
    \begin{split}
        M & = \begin{bmatrix}
            2 & 2 \\ 8 & 2
        \end{bmatrix} \\
        \lambda & = \left\{ -2, 6 \right\} \\
        u & = \left\{ \begin{bmatrix}
            -1 \\ 2
        \end{bmatrix},  \begin{bmatrix}
            1 \\ 2
        \end{bmatrix} \right\} \\
        \begin{bmatrix}
            2 & 2 \\ 8 & 2
        \end{bmatrix}\begin{bmatrix}
            -1 & 1 \\ 2 & 2
        \end{bmatrix} & = \begin{bmatrix}
            2 & 6 \\ -4 & 12
        \end{bmatrix} = \begin{bmatrix}
            -2\begin{bmatrix}
            -1 \\ 2
        \end{bmatrix} & 6\begin{bmatrix}
            1 \\ 2
        \end{bmatrix}
        \end{bmatrix}
    \end{split}
\end{equation}

\clearpage

\problem[\textit{Algebra} (Artin, 2e) Exercise 8.1.1 (extended)]
\subsection*{Part (a)}
Let $\langle v, w\rangle$ be a bilinear form on a real vector space $V$. Let $\varphi\left(v,  w\right) = \langle v, w \rangle + \langle w, v \rangle$. For arbitrary $r \in \R$, $v, w, v', w' \in V$:
\begin{equation}
    \begin{split}
        \varphi\left(v, w\right) & = \langle v, w \rangle + \langle w, v \rangle \\
        \varphi\left(rv, w\right) & = \langle rv, w \rangle + \langle w, rv \rangle \\
        & = r\langle v, w \rangle + r\langle w, v \rangle \\
        & = r\left(\langle  v, w \rangle + \langle w, v  \rangle\right) = r\varphi\left(v, w\right) \\
        \varphi\left(v, rw\right) & = \langle v, rw \rangle + \langle rw, v \rangle \\
        & = r\langle v, w \rangle + r\langle w, v \rangle \\
        & = r\left(\langle  v, w \rangle + \langle w, v  \rangle\right) = r\varphi\left(v, w\right) \\
        \varphi\left(v + v', w\right) & = \langle v + v', w \rangle + \langle w, v + v' \rangle \\
        & = \langle v, w \rangle + \langle v', w \rangle + \langle w, v \rangle + \langle w, v' \rangle \\
        & = \left(\langle  v, w \rangle + \langle w, v \rangle\right) + \left(\langle v', w \rangle + \langle w, v'  \rangle\right) = \varphi\left(v,  w) + \varphi(v', w\right) \\
        \varphi\left(v, w + w'\right) & = \langle v, w + w' \rangle + \langle w + w', v \rangle \\
        & = \langle v, w \rangle + \langle v, w' \rangle + \langle w, v \rangle + \langle w', v \rangle \\
        & = \left(\langle  v, w \rangle + \langle w, v \rangle) + (\langle v, w' \rangle + \langle w', v  \rangle\right) = \varphi\left(v,  w\right) + \varphi\left(v, w'\right) \\
    \end{split}
\end{equation}
Since the map $\varphi$ is bilinear in both variables, it is a bilinear form.
\subsection*{Part (b)}
For an arbitrary bilinear form $\langle v, w \rangle$, consider the following bilinear forms (which can be proven to be so similarly to the proof in part (a)):
\begin{equation}
    \begin{split}
        \varphi\left(v, w\right) & = \frac{1}{2}\left(\langle v, w \rangle + \langle w, v \rangle\right) \\
        \varphi\left(w, v\right) & = \frac{1}{2}\left(\langle  w, v \rangle + \langle v, w  \rangle\right) = \varphi\left(v, w\right) \\
        \phi\left(v, w\right) & = \frac{1}{2}\left(\langle  v, w \rangle - \langle w, v  \rangle\right) \\
        \phi\left(w, v\right) & = \frac{1}{2}\left(\langle  w, v \rangle - \langle v, w  \rangle\right) = -\phi\left(v, w\right) \\
    \end{split}
\end{equation}
Since $\varphi$ is evidently symmetric and $\phi$ skew-symmetric, and $\langle v, w\rangle = \varphi\left(v, w) + \phi(v, w\right)$, every bilinear form on a real vector space is the sum of a symmetric form and a skew symmetric form.

\clearpage

\problem[\textit{Algebra} (Artin, 2e) Exercise 8.3.4]
Let invertible matrix $A$. Then:
\begin{equation}
    \begin{split}
        \left(A^*A\right)^* & = A^*\left(A^*\right)^* \\
        & = A^*A
    \end{split}
\end{equation}
Since $\left(A^*A\right)^* = A^*A$, $A^*A$ is Hermitian. Let nonzero column vector $v$:
\begin{equation}
    \begin{split}
        v^*A^*Av & = \left(Av\right)^*\left(Av\right)
    \end{split}
\end{equation}
Since the nullspace of an invertible matrix is the zero vector, $w^*w > 0$ for all nonzero $w = Av$ and $v^*A^*Av = \left(Av\right)^*\left(Av\right) > 0$, which means that $A^*A$ is positive definite.

\clearpage

\problem
\subsection*{Part (a)}
Let $W$ be the subspace of $\R^3$ (with the standard dot product) that is generated by $v_1 = \begin{bmatrix} 1 \\ 1 \\ 0 \end{bmatrix}, v_2 = \begin{bmatrix} 0 \\ 1 \\ 1 \end{bmatrix}$. To find an orthogonal basis from $\left\{v_1, v_2\right\}$, we find $\left\{w_1, w_2 \right\}$ where $w_1 = v_1, w_2 = v_2 - \mathrm{proj}_{v_1} v_2$.
\begin{equation}
    \begin{split}
        v_1 & = \begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix}, v_2 = \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix} \\
        w_1 = v_1 & = \begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix} \\
        w_2 = v_2 - \mathrm{proj}_{v_1} v_2 & = \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix} - \frac{1}{2}\begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix} = \begin{bmatrix}-\frac{1}{2} \\ \frac{1}{2} \\ 1\end{bmatrix}
    \end{split}
\end{equation}
The orthogonal basis is $\left\{\begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix}, \begin{bmatrix}-\frac{1}{2} \\ \frac{1}{2} \\ 1\end{bmatrix}\right\}$.
\subsection*{Part (b)}
The projection of $\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ onto $W$ is:
\begin{equation}
    \begin{split}
        \mathrm{proj}_W \begin{bmatrix}
            0 \\ 1 \\ 0
        \end{bmatrix} & = \mathrm{proj}_{w_1} \begin{bmatrix}
            0 \\ 1 \\ 0
        \end{bmatrix} + \mathrm{proj}_{w_2} \begin{bmatrix}
            0 \\ 1 \\ 0
        \end{bmatrix} \\
        & = \frac{1}{2}\begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix} + \frac{\frac{1}{2}}{\frac{3}{2}}\begin{bmatrix}-\frac{1}{2} \\ \frac{1}{2} \\ 1\end{bmatrix} \\
        & = \begin{bmatrix}\frac{1}{3} \\ \frac{2}{3} \\ \frac{1}{3}\end{bmatrix}
    \end{split}
\end{equation}
\subsection*{Part (c)}
An alternative orthogonal basis can be found by changing the order of the vectors in the Gram-Schmidt procedure.
\begin{equation}
    \begin{split}
        v_1 & = \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}, v_2 = \begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix} \\
        w_1 = v_1 & = \begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix} \\
        w_2 = v_2 - \mathrm{proj}_{v_1} v_2 & = \begin{bmatrix}1 \\ 1 \\ 0\end{bmatrix} - \frac{1}{2}\begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix} = \begin{bmatrix}1 \\ \frac{1}{2} \\ -\frac{1}{2}\end{bmatrix}
    \end{split}
\end{equation}
The orthogonal basis is $\left\{\begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix}, \begin{bmatrix}1 \\ \frac{1}{2} \\ -\frac{1}{2}\end{bmatrix}\right\}$. The projection of $\begin{bmatrix} 0 \\ 1 \\ 0 \end{bmatrix}$ onto $W$ is:
\begin{equation}
    \begin{split}
        \mathrm{proj}_W \begin{bmatrix}
            0 \\ 1 \\ 0
        \end{bmatrix} & = \mathrm{proj}_{w_1} \begin{bmatrix}
            0 \\ 1 \\ 0
        \end{bmatrix} + \mathrm{proj}_{w_2} \begin{bmatrix}
            0 \\ 1 \\ 0
        \end{bmatrix} \\
        & = \frac{1}{2}\begin{bmatrix}0 \\ 1 \\ 1\end{bmatrix} + \frac{\frac{1}{2}}{\frac{3}{2}}\begin{bmatrix}1 \\ \frac{1}{2} \\ -\frac{1}{2}\end{bmatrix} \\
        & = \begin{bmatrix}\frac{1}{3} \\ \frac{2}{3} \\ \frac{1}{3}\end{bmatrix}
    \end{split}
\end{equation}
The result is the same, regardless of the orthogonal basis used.

\clearpage

\problem
Take the bilinear form with associated matrix of the form $A$ with respect to the standard $\R^3$ basis and subspace $W$ where:
\begin{equation}
    \begin{split}
        A & = \begin{bmatrix}
            1 & 0 & 0 \\ 0 & 1 & 1 \\ 0 & 1 & 1
        \end{bmatrix} \\
        W & = \{\begin{bmatrix}
            x \\ y \\ z
        \end{bmatrix} \in \R^3 \ : \ y = -z\}
    \end{split}
\end{equation}
However, for all $w \in W$:
\begin{equation}
    \begin{split}
        \begin{bmatrix}
            0 \\ 1 \\ -1
        \end{bmatrix}^TAw & = \begin{bmatrix}
            0 & 0 & 0
        \end{bmatrix}w = 0
    \end{split}
\end{equation}
The orthogonal complement is then $W^{\perp} = \left\{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} \in \R^3 \ : \ x = 0, y = -z\right\}$. Consequently, $W \cap W^{\perp} = \left\{ \begin{bmatrix} x \\ y \\ z \end{bmatrix} \in \R^3 \ : \ x = 0, y = -z \right\}$, which is one-dimensional.

\end{document}